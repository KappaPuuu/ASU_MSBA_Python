{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression\n",
    "\n",
    "- https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "\"Regular expressions (called *REs*, or *regexes*, or *regex* patterns) are essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module. Using this little language, you specify the rules for the set of possible strings that you want to match; this set might contain English sentences, or e-mail addresses, or TeX commands, or anything you like. You can then ask questions such as “Does this string match the pattern?”, or \n",
    "\n",
    "\n",
    "“Is there a match for the pattern anywhere in this string?”. You can also use REs to modify a string or to split it apart in various ways.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "   - **alphanumeric** here implies 0-9, a-z, A-Z, or _\n",
    "   - a word is defined as **a sequence of alphanumeric characters**\n",
    "\n",
    "### Metacharacters\n",
    "-   `[ ]`      matches character class specified within the square brackers \n",
    "  - `-` and `^` have special meaning within character class\n",
    "  -   `$` does not have special meaning within character class\n",
    "-   `-`      when used inside a characted class set, implies range of characters (*e.g. `[a-z]` , `[A-Z]`, `[0-9]`*)\n",
    "-   `^`      when used as first character inside a character class set, implies match of complementing character class set\n",
    "-   `\\`       is used to either escape a metacharacter of its special meaning, or to signify a special squence\n",
    "-   `.`        matches anything except a newline character\n",
    "-   `*`       previous character is matched **0 or more times**\n",
    "-   `+`      previous character is matched **1 or more times**\n",
    "-   `?`       previous characer is mathced **0 or 1 times**\n",
    "-   `{ }`     `{m,n}` means there must be at least m repetitions, and at most n \n",
    "  - `{0,}` is the same as `*`,\n",
    "  - `{1,}` is equivalent to `+`, and \n",
    "  - `{0,1}` is the same as `?`\n",
    "-   `^`     when **NOT** used as first character inside a character class set, matches at the begining of a line (*e.g. `[^a-z]` -> not start with a-z, `^why` -> sentense not begin with why)*\n",
    "-   `\\A`    matches only at the start of a string (equivalent to `^` in non-MULTILINE mode)\n",
    "-   `$`     matches at the end of a line\n",
    "-   `\\Z`    matches only at the end of a string (equivalent to `$` in non-MULTILINE mode)\n",
    "-   `\\b`    matches only at the begining or end of a word (that is, **at a word boundary**) *(e.g. catch `\\bcat\\b` -> no matches)*\n",
    "-   `\\B`    matches only when not at the begining or end of a word (that is, **not at a word boundary**)\n",
    "-   `|`      matches **either/or** expression on either side of | opeartor\n",
    "-   `( )`    used to **group** together the expressions contained inside; <br> you can then repeat the contents of a group with a repeating qualifier, such as `*`, `+`, `?`, or `{m,n}`\n",
    "\n",
    "### Special Squences (all sequencces can be included in a character set)\n",
    "-   `\\d`    matches any **digit** character; equivalent to `[0-9]`\n",
    "-   `\\D`    matches any **non-digit** character; equivalent to `[^0-9]`\n",
    "-   `\\s`     matches any **whitespace character**; equivalent to `[ \\t\\n\\r\\f\\v]` => space, tab, newline, carriage return, form feed, vertical tab\n",
    "-   `\\S`     matches any **non-whitespace** character; equivalent to `[^\\t\\n\\r\\f\\v]`\n",
    "-   `\\w`    matches any **alphanumeric** character; equivalent to `[0-9a-zA-Z_]`\n",
    "-   `\\W`    matches any **non-alphanumeric** character; equivalent to `[^0-9a-zA-Z_]`\n",
    "\n",
    "### Raw Strings\n",
    "- Regular expressions use the backslash character (`'\\'`) to indicate special forms or to allow special characters to be used without invoking their special meaning. <br>\n",
    "- This conflicts with Python’s usage of the same character for the same purpose in string literals.<br>\n",
    "- The solution is to use Python’s raw string notation for regular expressions.<br>\n",
    "- This is done by preceeding the regular expression pattern by `r\"..\"` (raw string mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expressions are compiled into pattern objects:\n",
    "#    import re\n",
    "#    regex = re.compile(pattern, options)\n",
    "#        - pattern: created using metacharacters and special squences\n",
    "#        - options: can be re.IGNORECASE, re.VERBOSE, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once a pattern object is created, you can use one of several methods on it to create a match object\n",
    "\n",
    "# match(): determines if the pattern matches at the begining of the string\n",
    "# search(): determines if the pattern matches at any location of the string\n",
    "# findall(): find all substrings where pattern matches, and return them as a list\n",
    "# finditer(): find all substrings where pattern matches, and return them as an iterator\n",
    "\n",
    "# Once a match object is created,  you can query the match object for information about the matching string\n",
    "# group(): returns string matched by the pattern\n",
    "# start(): return starting position of the match\n",
    "# end(): return ending position of the match\n",
    "# span(): return a tuple containing (start, end) position of the match\n",
    "\n",
    "# Once a pattern object is created, you can also use the following methods to modify strings\n",
    "\n",
    "# split(string[, maxsplit=0]): \n",
    "#               split the string into a list, splitting wherever the pattern matches \n",
    "#               if maxsplit is non-zero, at most maxsplit splits are performed (otherwise all splits are done)\n",
    "\n",
    "# sub(replacement, string[, count=0]):  ### <- most often, take unstructured data and clean up\n",
    "#               find all substrings where the pattern matches, and replace them with a different string\n",
    "#               if count is non-zero, at most count replacements are performed (otherwise all replacements are done)\n",
    "\n",
    "# subn(): same as sub, but returns new string and number of replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 \n",
    "replace Character ['a', '!', '1'] with hash '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey# Are we still on for lunch tod#y #t ###m?\n"
     ]
    }
   ],
   "source": [
    "# string\n",
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# replace Character ['a', '!', '1'] with hash '#'\n",
    "regex = re.compile(r\"[a!1]\")\n",
    "\n",
    "# replace with '#'\n",
    "newstr = regex.sub('#',oldstr)\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "covert [a-z] to hash'#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H##! A## ## ##### ## ### ##### ##### ## 11##?\n"
     ]
    }
   ],
   "source": [
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# covert [a-z] to hash'#'\n",
    "\n",
    "regex = re.compile(r\"[a-z]\")\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3\n",
    "covert all a-zA-Z charaters to hash'#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###! ### ## ##### ## ### ##### ##### ## 11##?\n"
     ]
    }
   ],
   "source": [
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# covert [a-zA-Z] to hash'#'\n",
    "\n",
    "regex = re.compile(r\"[a-zA-Z]\")\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###! ### ## ##### ## ### ##### ##### ## 11##?\n"
     ]
    }
   ],
   "source": [
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# covert [a-zA-Z] to hash'#'\n",
    "\n",
    "regex = re.compile(r\"[a-z]\", re.IGNORECASE)\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4\n",
    "change all digit to hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! Are we still on for lunch today at ##am?\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "regex = re.compile(r\"[0-9]\")\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! Are we still on for lunch today at ##am?\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# special sequences\n",
    "regex = re.compile(r\"[\\d]\")\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey##Are#we#still#on#for#lunch#today#at###am#\n"
     ]
    }
   ],
   "source": [
    "# method 3 (include the spaces and marks)\n",
    "\n",
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# special sequences\n",
    "regex = re.compile(r\"[^a-zA-Z]\")\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################11###\n"
     ]
    }
   ],
   "source": [
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# special sequences\n",
    "regex = re.compile(r\"[^\\d]\")\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey!#Are#we#still#on#for#lunch#today#at#11am?\n"
     ]
    }
   ],
   "source": [
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "\n",
    "# special sequences # spaces\n",
    "regex = re.compile(r\"[\\s]\")\n",
    "\n",
    "newstr = regex.sub('#',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Lisa, #, #\n"
     ]
    }
   ],
   "source": [
    "oldstr = \"Why Lisa, why, WHY\"\n",
    "\n",
    "# replace all y to '#'\n",
    "regex = re.compile(r\"why\", re.IGNORECASE)\n",
    "\n",
    "newstr = regex.sub('#', oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Lisa, why, WHY\n"
     ]
    }
   ],
   "source": [
    "# beginning of the line\n",
    "oldstr = \"Why Lisa, why, WHY\"\n",
    "\n",
    "regex = re.compile(r\"^why\", re.IGNORECASE)\n",
    "\n",
    "newstr = regex.sub('#', oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why Lisa, why, #\n"
     ]
    }
   ],
   "source": [
    "# endding of the line\n",
    "oldstr = \"Why Lisa, why, WHY\"\n",
    "\n",
    "regex = re.compile(r\"why$\", re.IGNORECASE)\n",
    "\n",
    "newstr = regex.sub('#', oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the # will #ch-up with you in mus#\n"
     ]
    }
   ],
   "source": [
    "# word boundary\n",
    "\n",
    "oldstr = \"the cat will catch-up with you in muscat\"\n",
    "\n",
    "regex = re.compile(r\"cat\", re.IGNORECASE)\n",
    "newstr = regex.sub('#', oldstr)\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the # will catch-up with you in muscat\n"
     ]
    }
   ],
   "source": [
    "# word boundary - full word\n",
    "oldstr = \"the cat will catch-up with you in muscat\"\n",
    "\n",
    "regex = re.compile(r\"\\bcat\\b\", re.IGNORECASE)\n",
    "newstr = regex.sub('#', oldstr)\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the # will #ch-up with you in muscat\n"
     ]
    }
   ],
   "source": [
    "# word boundary - begins with cat\n",
    "oldstr = \"the cat will catch-up with you in muscat\"\n",
    "\n",
    "regex = re.compile(r\"\\bcat\", re.IGNORECASE)\n",
    "newstr = regex.sub('#', oldstr)\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the # will catch-up with you in mus#\n"
     ]
    }
   ],
   "source": [
    "# word boundary - ends with cat\n",
    "oldstr = \"the cat will catch-up with you in muscat\"\n",
    "\n",
    "regex = re.compile(r\"cat\\b\", re.IGNORECASE)\n",
    "newstr = regex.sub('#', oldstr)\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "#### Exercise 1: \n",
    " - find file names of the form base.extension  \n",
    " - and print the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two files are foo1.bar and foo2.bar. There are no other files.\n"
     ]
    }
   ],
   "source": [
    "fnamestr = \"The two files are foo1.bar and foo2.bar. There are no other files.\"\n",
    "print (fnamestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foo1.bar', 'foo2.bar']\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"\\b\\w+[.]\\w+\\b\", re.IGNORECASE)\n",
    "\n",
    "fnames = regex.findall(fnamestr)\n",
    "print(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    " - find punctuations and digits\n",
    " - replace with empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! Are we still on for lunch today at 11am?\n"
     ]
    }
   ],
   "source": [
    "oldstr = \"Hey! Are we still on for lunch today at 11am?\"\n",
    "print (oldstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Are we still on for lunch today at am\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "regex = re.compile(r\"[%s%s]\"%(string.digits, string.punctuation)) ### <-\n",
    "\n",
    "newstr = regex.sub('',oldstr)\n",
    "\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "- https://spacy.io/\n",
    "- https://spacy.io/usage\n",
    "- https://spacy.io/models/en\n",
    "- https://spacy.io/api/doc\n",
    "- https://spacy.io/api/token\n",
    "- https://spacy.io/usage/processing-pipelines\n",
    "- https://spacy.io/usage/spacy-101\n",
    "\n",
    "\n",
    "\n",
    " - **spaCy** is a free, open-source library for advanced industrial-strength Natural Language Processing (NLP) in Python.\n",
    "\n",
    "- When you call spaCy on a text, spaCy first tokenizes the text (i.e. segments it into words, punctuation and so on) to produce a Doc object. \n",
    "   spaCy uses rules specific to each language for tokenization.\n",
    "\n",
    " - The Doc object is then processed in several different steps (also referred to as the processing pipeline). \n",
    "   The pipeline used by the default models consists of a (pos) tagger, a (dependency) parser and a (named) entity recognizer (ner). \n",
    "   spaCy uses statistical models to predict pos, syntatctic dependencies, and named entities.\n",
    "   Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "   You can pick and choose the stages you want spaCy to load.\n",
    "\n",
    "- Here is a list of features and capabilities of spaCy: \n",
    "  https://spacy.io/usage/spacy-101#features\n",
    "\n",
    "### installation\n",
    "\n",
    "#### https://spacy.io/usage\n",
    "- `pip install spacy`\n",
    "\n",
    "#### https://spacy.io/models/en\n",
    "you can download these general-purpose pretrained models to predict \n",
    "pos tags (tagger), named entities (ner), and syntactic dependencies (parser).\n",
    "    note: n_core_web_sm does not include **word-vectors**, but en_core_web_md and en_core_web_lg do.\n",
    "> `python -m spacy download en_core_web_sm` <br>\n",
    "> `python -m spacy download en_core_web_md` <br>\n",
    "> `python -m spacy download en_core_web_lg`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you’ve downloaded and installed a model, you can load it via spacy.load(). \n",
    "# spacy.load() returns a Language object containing all components and data needed to process text. \\\n",
    "\n",
    "# the Language object is typically called nlp. \n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the nlp object on a string of text will return a processed Doc object. the Doc object is typically called doc.\n",
    "# even though a Doc object is processed (for isntance, split into individual words and annotated),\n",
    "# it still holds all information of the original text.\n",
    "# once the doc object has been created, we can  use it to access the various spaCy features.\n",
    "doc = nlp(\"Hi Emma Watson! How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Emma Watson!\n",
      "How are you?\n"
     ]
    }
   ],
   "source": [
    "# for instance, you can iterate over individual sentences in the document.\n",
    "for s in doc.sents:\n",
    "    print (s.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watson\n",
      "PERSON\n",
      "People, including fictional\n"
     ]
    }
   ],
   "source": [
    "# you can iterate over the named entities in the document (from ner)\n",
    "# a named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title.\n",
    "for e in doc.ents:\n",
    "    print (e.text)\n",
    "    print (e.label_)\n",
    "    print (spacy.explain(e.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hi Emma \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Watson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "! How are you?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you can visualize the named entities \n",
    "spacy.displacy.render(doc, style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"090c8a9a4555498da9eefccc626fb372-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Hi</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Emma</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Watson!</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">How</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">you?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-090c8a9a4555498da9eefccc626fb372-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-090c8a9a4555498da9eefccc626fb372-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-090c8a9a4555498da9eefccc626fb372-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-090c8a9a4555498da9eefccc626fb372-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-090c8a9a4555498da9eefccc626fb372-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-090c8a9a4555498da9eefccc626fb372-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-090c8a9a4555498da9eefccc626fb372-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-090c8a9a4555498da9eefccc626fb372-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you can also visualize the dependencies (from parser)\n",
    "spacy.displacy.render(doc, style=\"dep\", jupyter= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Emma Watson\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "# you can iterate over the base noun chunks in the document.\n",
    "# noun chunks are “base noun phrases”  - a noun plus the words describing the noun.\n",
    "# for instance, “the lavish green grass” or “the world’s largest tech fund”.\n",
    "for c in doc.noun_chunks:\n",
    "    print (c.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Hi Hi  None hi INTJ interjection UH interjection compound False False False False True True False False False True False False False\n",
      "1 Emma Emma  None Emma PROPN proper noun NNP noun, proper singular compound False False False False True True False False False True False False False\n",
      "2 Watson Watson PERSON People, including fictional Watson PROPN proper noun NNP noun, proper singular ROOT False False False False True True False False False True False False False\n",
      "3 ! !  None ! PUNCT punctuation . punctuation mark, sentence closer punct False False False False False True False False False False True False False\n",
      "4 How How  None how ADV adverb WRB wh-adverb advmod False False False True True True False False False True False False False\n",
      "5 are are  None be AUX auxiliary VBP verb, non-3rd person singular present ROOT False False False True True True False True False False False False False\n",
      "6 you you  None -PRON- PRON pronoun PRP pronoun, personal nsubj False False False True True True False True False False False False False\n",
      "7 ? ?  None ? PUNCT punctuation . punctuation mark, sentence closer punct False False False False False True False False False False True False False\n"
     ]
    }
   ],
   "source": [
    "# you can iterate over the linguisitic annotations associated with tokens in the document (from tagger)\n",
    "# https://spacy.io/api/annotation\n",
    "# https://spacy.io/api/token#attributes\n",
    "doc = nlp(\"Hi Emma Watson! How are you?\")\n",
    "for token in doc:\n",
    "    print (token.i,                  # index of the token within the parent document\n",
    "           token,\n",
    "           token.text,               # verbatim text\n",
    "           token.ent_type_,          # named entity type\n",
    "           spacy.explain(token.ent_type_),\n",
    "           token.lemma_,             # base form of the token, with no inflectional suffixes\n",
    "           token.pos_,               # coarse-grained part-of-speech\n",
    "            spacy.explain(token.pos_),\n",
    "           token.tag_,               # fine-grained part-of-speech\n",
    "            spacy.explain(token.tag_),\n",
    "           token.dep_,               # syntactic dependency relation\n",
    "           token.like_url,           # does the token resemble a URL\n",
    "           token.like_num,           # does the token represent a number? e.g. “10.9”, “10”, “ten”, etc\n",
    "           token.like_email,         # does the token resemble an email address\n",
    "           token.is_stop,            # is the token part of a “stop list”\n",
    "          token.is_alpha,\n",
    "          token.is_ascii,\n",
    "          token.is_digit,\n",
    "          token.is_lower,\n",
    "          token.is_upper,\n",
    "          token.is_title,\n",
    "          token.is_punct,\n",
    "          token.is_space,\n",
    "          token.is_currency\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-abf7efea721c>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  doc1.similarity(doc2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9100590601885606"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can make semantic similarity estimates based on word vectors.\n",
    "# the default estimate is cosine similarity, using an average of word vectors for the document.\n",
    "# it returns a scalar similarity score (higher is more similar).\n",
    "doc1 = nlp(\"I like oranges that are sweet.\")\n",
    "# print (doc1.vector) # doc vector is average of token vectors\n",
    "doc2 = nlp(\"I like apples that are sour.\")\n",
    "# print (doc2.vector) # doc vector is average of token vectors\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amy go class', 'matt have lunch']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing large corpuses with nlp.pipe\n",
    "\n",
    "# let's say you had a very large corpus of text\n",
    "# illustrated with a very small corpus below :)\n",
    "data = [\"Amy is going to class now.\",\n",
    "          \"Matt is having lunch.\"]\n",
    "\n",
    "# first, you'll only want to apply the pipeline components you need:\n",
    "# getting predictions from the model that you don’t actually need adds up and becomes very inefficient at scale. \n",
    "# to prevent this, use the disable keyword argument to disable components you don’t need.\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# and second, you'll want to work on batches of texts.\n",
    "# this can be done with spaCy’s nlp.pipe method which takes an iterable of texts and yields processed Doc objects. \n",
    "# the batching is done internally.\n",
    "corpus = nlp.pipe(data)\n",
    "\n",
    "# now we can clean the corpus efficiently\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = [token.lemma_.lower() \n",
    "                      for token in doc \n",
    "                          if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "clean_corpus = [custom_tokenizer(doc) for doc in corpus]\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise\n",
    "\n",
    "#### Question 1\n",
    "Lowercase the text.\n",
    "- `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes, that is a duplicate catalog category. the catalog number is c1357-a.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"\n",
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Substitute the pattern `\"cat\"` with replacement `\"#\"` in the text.\n",
    "- make the substitution case sensitive,\n",
    "- and match the pattern wherever it occurs in the text\n",
    "  - `text = \"Yes, that is a duplicate Catalog category. The Catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, that is a dupli#e #alog #egory. The #alog number is C1357-A.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r\"cat\")\n",
    "newtext = regex.sub('#', text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Substitute the pattern \"cat\" with replacement \"#\" in the text\n",
    "- make the substitution case insensitive (re.IGNORECASE)\n",
    "- and match the pattern wherever it occurs in the text\n",
    "  - `text = \"Yes, that is a duplicate Catalog category. The Catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, that is a dupli#e #alog #egory. The #alog number is C1357-A.\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"cat\", re.IGNORECASE)\n",
    "newtext = regex.sub('#', text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Substitute the pattern \"cat\" with replacement \"#\" in the text\n",
    "- make the substitution case insensitive\n",
    "- only match if the pattern is at the beginning of a word boundary (\\b)\n",
    "  - `text = \"Yes, that is a duplicate Catalog category. The Catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, that is a duplicate #alog #egory. The #alog number is C1357-A.\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"\\bcat\", re.IGNORECASE)\n",
    "newtext = regex.sub('#', text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "Substitute the characters 'c', 'a', 't' with replacement \"#\" in the text\n",
    "- make the substitution case insensitive\n",
    "- hint: use character class r\"[cat]“\n",
    "  - `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, #h## is # dupli###e ####log ###egory. #he ####log number is #1357-#.\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"[cat]\", re.IGNORECASE)\n",
    "newtext = regex.sub('#', text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "Substitute all alphabets with replacement \"#\" in the text\n",
    "- make the substitution case insensitive\n",
    "- hint: use character class with range [a-z]\n",
    "  - `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###, #### ## # ######### ####### ########. ### ####### ###### ## #1357-#.\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"[a-z]\", re.IGNORECASE)\n",
    "newtext = regex.sub('#', text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "Substitute all digits with replacement \"#\" in the text\n",
    "- hint: use character class with range [0-9], or special sequence \\d = [0-9]\n",
    "  - `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, that is a duplicate catalog category. The catalog number is C####-A.\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"[0-9]\")\n",
    "newtext = regex.sub('#', text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "Substitute one or more white-space characters with replacement \" \" in the spacetext\n",
    "- hint: use special sequence \\s for whitespace characters, and metacharacter + for one-or-moretimes\n",
    "  - `spacetext = \"Yes, that is a duplicate catalog \\t category. The catalog number is C1357-A.\\n\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, that is a duplicate catalog category. The catalog number is C1357-A. \n"
     ]
    }
   ],
   "source": [
    "spacetext = \"Yes, that is a duplicate catalog \\t category. The catalog number is C1357-A.\\n\"\n",
    "\n",
    "regex = re.compile(r\"\\s+\")\n",
    "newtext = regex.sub(' ', spacetext)\n",
    "# print(spacetext)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "Substitute words that are two or more alphanumeric characters long\n",
    "with replacement \"#\"\n",
    "- use special sequence \\w for alphanumeric characters [0-9a-zA-Z_],\n",
    "- special sequence \\b for words boundaries,\n",
    "- and metacharacter + for one-or-more-times, or {2,} for two or more times\n",
    "  - `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#, # # a # # #. # # # # #-A.\n"
     ]
    }
   ],
   "source": [
    "text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"\n",
    "\n",
    "regex = re.compile(r\"[\\w]{2,}\")\n",
    "newtext = regex.sub('#', text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "Find all words that are two or more characters long\n",
    "- hint: use regex.findall(text)\n",
    "  - `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'that', 'is', 'duplicate', 'catalog', 'category', 'The', 'catalog', 'number', 'is', 'C1357']\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r\"\\b\\w{2,}\\b\")\n",
    "newtext = regex.findall(text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11 (@20:00)\n",
    "Find all the urls in urltext\n",
    "- take care of http vs https\n",
    "- hint: use metacharacter ? for zero-or-one-times, metacharacter + for one-or-more-times, and \\S for non-whitespace characters\n",
    "  - `urltext = \n",
    "  \"\"\"The url for sklearn documentation is https://scikit-learn.org/stable/. \n",
    "You can learn more about pipelines by following these links: https://scikit-learn.org/stable/modules/compose.html\n",
    "and https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\"\"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://scikit-learn.org/stable',\n",
       " 'https://scikit-learn.org/stable/modules/compose.html',\n",
       " 'https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urltext = \"\"\"The url for sklearn documentation is https://scikit-learn.org/stable/. \n",
    "You can learn more about pipelines by following these links: https://scikit-learn.org/stable/modules/compose.html\n",
    "and https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\"\"\"\n",
    "\n",
    "regex = re.compile(r\"\\bhttps?://[\\S]+\\b\")\n",
    "urls = regex.findall(urltext)\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Set 2\n",
    "String formatting is useful when we're trying to create patterns on the fly:\n",
    "> `print (string.digits)\n",
    "strd = r\"[%s]\" % string.digits print (strd)\n",
    "print ()`\n",
    "\n",
    "\n",
    "> `print (string.punctuation)\n",
    "strp = r\"[%s]\" % string.punctuation print (strp)\n",
    "print ()`\n",
    "\n",
    "\n",
    "> `strpd = r\"%s%s\" % (string.punctuation, string.digits)\n",
    "print (strpd)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789\n",
      "[0123456789]\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "[!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print (string.digits)\n",
    "strd = r\"[%s]\" % string.digits\n",
    "print (strd)\n",
    "print ()\n",
    "\n",
    "print (string.punctuation)\n",
    "strp = r\"[%s]\" % string.punctuation\n",
    "print (strp)\n",
    "print ()\n",
    "\n",
    "strpd = r\"%s%s\" % (string.punctuation, string.digits)\n",
    "print (strpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12\n",
    "\n",
    "Substitute all punctuations with replacement \"#\" in the text\n",
    "- hint: use string formatting and string.punctuation to create the required character class\n",
    "  - `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes# that is a duplicate catalog category# The catalog number is C1357#A#\n"
     ]
    }
   ],
   "source": [
    "text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"\n",
    "\n",
    "regex = re.compile(r\"[%s]\" % string.punctuation)\n",
    "newtext = regex.sub(\"#\", text)\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASU Sun Devils\n",
      "['ASU', 'Sun', 'Devils']\n",
      "ASU Sun Devils\n"
     ]
    }
   ],
   "source": [
    "# Notice that split and join work in opposite ways\n",
    "st = \"ASU Sun Devils\"\n",
    "print (st)\n",
    "# “split” splits the string (at ' ' by default), and put the words in a list \n",
    "stsplit = st.split() # splitting at ' ' by default\n",
    "print (stsplit)\n",
    "# “join” joins the element of the list using the separator specified \n",
    "stjoin = ' '.join(stsplit) # joining using ' '\n",
    "print (stjoin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords\n",
    "\n",
    "- Stop words are a set of commonly used words in any language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "print (stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13\n",
    "Remove stopwords from the text\n",
    "- hint: use lower, split, list comprehension, and join\n",
    "  - `text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.“`\n",
    "  - `sw = stopwords.words('english')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes, duplicate catalog category. catalog number c1357-a.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Yes, that is a duplicate catalog category. The catalog number is C1357-A.\"\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "' '.join([i.lower() for i in text.split() if i.lower() not in sw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14\n",
    "Lemmatization attempts to get the word root through vocabulary and morphological analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolf\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "text = 'wolves'\n",
    "print(WordNetLemmatizer().lemmatize('wolves'))\n",
    "\n",
    "# or insatantiate wnl = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wolv'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "text = 'wolves'\n",
    "PorterStemmer().stem(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing a text corpus using regex and nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called preprocess(txt) that pre-processes the txt that's been passed in:\n",
    "- lower case,\n",
    "- remove digits,\n",
    "- remove punctuation,\n",
    "- remove extra white-spaces,\n",
    "- remove stop words,\n",
    "- remove words < 2 characters long (i.e., retain words >= 2 characters long),\n",
    "- lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the function to preprocess the doc\n",
    "- `doc = preprocess(doc)\n",
    "doc`\n",
    "\n",
    "\n",
    "use the function to preprocess docs in corpus2\n",
    "- `clean_corpus2 = [preprocess(text) for text in corpus2]\n",
    "clean_corpus2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python',\n",
       " 'interpreted',\n",
       " 'highlevel',\n",
       " 'generalpurpose',\n",
       " 'programming',\n",
       " 'language',\n",
       " 'created',\n",
       " 'guido',\n",
       " 'van',\n",
       " 'rossum',\n",
       " 'first',\n",
       " 'released',\n",
       " 'python',\n",
       " 'design',\n",
       " 'philosophy',\n",
       " 'emphasizes',\n",
       " 'code',\n",
       " 'readability',\n",
       " 'notably',\n",
       " 'using',\n",
       " 'significant',\n",
       " 'whitespace',\n",
       " 'provides',\n",
       " 'construct',\n",
       " 'enable',\n",
       " 'clear',\n",
       " 'programming',\n",
       " 'small',\n",
       " 'large',\n",
       " 'scale',\n",
       " 'van',\n",
       " 'rossum',\n",
       " 'led',\n",
       " 'language',\n",
       " 'community',\n",
       " 'stepping',\n",
       " 'leader',\n",
       " 'july',\n",
       " 'python',\n",
       " 'feature',\n",
       " 'dynamic',\n",
       " 'type',\n",
       " 'system',\n",
       " 'automatic',\n",
       " 'memory',\n",
       " 'management',\n",
       " 'support',\n",
       " 'multiple',\n",
       " 'programming',\n",
       " 'paradigm',\n",
       " 'including',\n",
       " 'objectoriented',\n",
       " 'imperative',\n",
       " 'functional',\n",
       " 'procedural',\n",
       " 'large',\n",
       " 'comprehensive',\n",
       " 'standard',\n",
       " 'library',\n",
       " 'python',\n",
       " 'interpreter',\n",
       " 'available',\n",
       " 'many',\n",
       " 'operating',\n",
       " 'system',\n",
       " 'cpython',\n",
       " 'reference',\n",
       " 'implementation',\n",
       " 'python',\n",
       " 'open',\n",
       " 'source',\n",
       " 'software',\n",
       " 'communitybased',\n",
       " 'development',\n",
       " 'model',\n",
       " 'nearly',\n",
       " 'python',\n",
       " 'implementation',\n",
       " 'python',\n",
       " 'cpython',\n",
       " 'managed',\n",
       " 'nonprofit',\n",
       " 'python',\n",
       " 'software',\n",
       " 'foundation']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"\"\"Python is an interpreted, high-level, general-purpose programming \n",
    "language. Created by Guido van Rossum and first released in 1991, Python has a \n",
    "design philosophy that emphasizes code readability, notably using significant \n",
    "whitespace. It provides constructs that enable clear programming on both small \n",
    "and large scales.[26] Van Rossum led the language community until stepping \n",
    "down as leader in July 2018.[27][28] Python features a dynamic type system \n",
    "and automatic memory management. It supports multiple programming paradigms, \n",
    "including object-oriented, imperative, functional and procedural, and has a \n",
    "large and comprehensive standard library.[29] Python interpreters are \n",
    "available for many operating systems. CPython, the reference implementation of \n",
    "Python, is open source software[30] and has a community-based development \n",
    "model, as do nearly all of Python's other implementations. Python and CPython \n",
    "are managed by the non-profit Python Software Foundation.\"\"\"\n",
    "\n",
    "preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess (txt):\n",
    "    \n",
    "    txt = txt.lower()\n",
    "    \n",
    "    txt = re.compile(r\"\\d\").sub(' ',txt)\n",
    "    \n",
    "    txt = re.compile(r\"[%s]\" %string.punctuation).sub(' ',txt)\n",
    "    txt = re.compile(r\"\\s+\").sub(' ',txt)\n",
    "    \n",
    "    sw = stopwords.words('english')\n",
    "    txt = txt.split()\n",
    "    txt = ' '.join([w for w in txt if w not in sw])\n",
    "    \n",
    "    regex = re.compile(r\"\\b\\w{2,}\\b\")\n",
    "    txt = ' '.join(regex.findall(txt))\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    txt = ' '.join([wnl.lemmatize(w) for w in txt.split()])\n",
    "    \n",
    "    return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python interpreted high level general purpose programming language created guido van rossum first released python design philosophy emphasizes code readability notably using significant whitespace provides construct enable clear programming small large scale van rossum led language community stepping leader july python feature dynamic type system automatic memory management support multiple programming paradigm including object oriented imperative functional procedural large comprehensive standard library python interpreter available many operating system cpython reference implementation python open source software community based development model nearly python implementation python cpython managed non profit python software foundation'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = [\"This is a brown house. This house is big.\",\n",
    "          \"This is a small house. This house has 1 bedroom.\",\n",
    "          \"This dog is brown. This dog likes to play\",\n",
    "          \"The dog is in the bedroom.\"]\n",
    "\n",
    "document1 = \"\"\"In Greek mythology, Python (Greek: Πύθων, gen.: Πύθωνος) was the earth-dragon of \n",
    "Delphi, always represented in Greek sculpture and vase-paintings as a serpent. He presided at the \n",
    "Delphic oracle, which existed in the cult center for his mother, Gaia, \"Earth,\" Pytho being the \n",
    "place name that was substituted for the earlier Krisa.[1] Hellenes considered the site to be the \n",
    "center of the earth, represented by a stone, the omphalos or navel, which Python guarded.\"\"\"\n",
    "\n",
    "document2 = \"\"\"Monty Python (sometimes known as The Pythons)[2][3] were a British surreal comedy \n",
    "group who created the sketch comedy show Monty Python's Flying Circus, that first aired on the BBC on \n",
    "October 5, 1969. Forty-five episodes were made over four series. The Python phenomenon developed from \n",
    "the television series into something larger in scope and impact, spawning touring stage shows, films, \n",
    "numerous albums, several books, and a stage musical. The group's influence on comedy has been compared \n",
    "to The Beatles' influence on music.\"\"\"\n",
    "\n",
    "document3 = \"\"\"Python is a widely used general-purpose, high-level programming language.[19][20] \n",
    "Its design philosophy emphasizes code readability, and its syntax allows programmers to express \n",
    "concepts in fewer lines of code than would be possible in languages such as C++ or Java.[21][22] \n",
    "The language provides constructs intended to enable clear programs on both a small and large scale.\"\"\"\n",
    "\n",
    "corpus2 = [document1, document2, document3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek mythology python greek πύθων gen πύθωνος earth dragon delphi always represented greek sculpture vase painting serpent presided delphic oracle existed cult center mother gaia earth pytho place name substituted earlier krisa hellene considered site center earth represented stone omphalos navel python guarded',\n",
       " 'monty python sometimes known python british surreal comedy group created sketch comedy show monty python flying circus first aired bbc october forty five episode made four series python phenomenon developed television series something larger scope impact spawning touring stage show film numerous album several book stage musical group influence comedy compared beatles influence music',\n",
       " 'python widely used general purpose high level programming language design philosophy emphasizes code readability syntax allows programmer express concept fewer line code would possible language java language provides construct intended enable clear program small large scale']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus2 = [preprocess(txt) for txt in corpus2]\n",
    "clean_corpus2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(doc):\n",
    "    tokens = [token.lemma_.lower()\n",
    "             for token in doc\n",
    "                if (\n",
    "                    len(token >= 2) and\n",
    "                    not token.is_punct and\n",
    "                    not token.is_space and\n",
    "                    not token.is_stop and\n",
    "                    not token.is_digit)]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"This is a brown house. This house is big.\",\n",
    "          \"This is a small house. This house has 1 bedroom.\",\n",
    "          \"This dog is brown. This dog likes to play\",\n",
    "          \"The dog is in the bedroom.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got int)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-d2d767268bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclean_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-126-d2d767268bb3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclean_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-cd98435ed434>\u001b[0m in \u001b[0;36mcustom_tokenizer\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     tokens = [token.lemma_.lower()\n\u001b[0m\u001b[1;32m      3\u001b[0m              \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 if (\n\u001b[1;32m      5\u001b[0m                     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-cd98435ed434>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 if (\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_space\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got int)"
     ]
    }
   ],
   "source": [
    "nlp_corpus = nlp.pipe(corpus)\n",
    "clean_corpus = [custom_tokenizer(doc) for doc in nlp_corpus]\n",
    "clean_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown house house big',\n",
       " 'small house house bedroom',\n",
       " 'dog brown dog like play',\n",
       " 'dog bedroom']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(map(preprocess, corpus))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brown', 'house', 'house', 'big'],\n",
       " ['small', 'house', 'house', 'bedroom'],\n",
       " ['dog', 'brown', 'dog', 'like', 'play'],\n",
       " ['dog', 'bedroom']]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [w.split() for w in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0    brown\n",
       " 1    house\n",
       " 2    house\n",
       " 3      big\n",
       " dtype: object,\n",
       " 0      small\n",
       " 1      house\n",
       " 2      house\n",
       " 3    bedroom\n",
       " dtype: object,\n",
       " 0      dog\n",
       " 1    brown\n",
       " 2      dog\n",
       " 3     like\n",
       " 4     play\n",
       " dtype: object,\n",
       " 0        dog\n",
       " 1    bedroom\n",
       " dtype: object]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "corpus = [pd.Series(i) for i in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[house    2\n",
       " big      1\n",
       " brown    1\n",
       " dtype: int64,\n",
       " house      2\n",
       " small      1\n",
       " bedroom    1\n",
       " dtype: int64,\n",
       " dog      2\n",
       " like     1\n",
       " play     1\n",
       " brown    1\n",
       " dtype: int64,\n",
       " dog        1\n",
       " bedroom    1\n",
       " dtype: int64]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [i.value_counts() for i in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house</th>\n",
       "      <th>big</th>\n",
       "      <th>brown</th>\n",
       "      <th>small</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house  big  brown  small  bedroom  dog  like  play\n",
       "0    2.0  1.0    1.0    NaN      NaN  NaN   NaN   NaN\n",
       "1    2.0  NaN    NaN    1.0      1.0  NaN   NaN   NaN\n",
       "2    NaN  NaN    1.0    NaN      NaN  2.0   1.0   1.0\n",
       "3    NaN  NaN    NaN    NaN      1.0  1.0   NaN   NaN"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=corpus)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house</th>\n",
       "      <th>big</th>\n",
       "      <th>brown</th>\n",
       "      <th>small</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house  big  brown  small  bedroom  dog  like  play\n",
       "0    2.0  1.0    1.0    0.0      0.0  0.0   0.0   0.0\n",
       "1    2.0  0.0    0.0    1.0      1.0  0.0   0.0   0.0\n",
       "2    0.0  0.0    1.0    0.0      0.0  2.0   1.0   1.0\n",
       "3    0.0  0.0    0.0    0.0      1.0  1.0   0.0   0.0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
