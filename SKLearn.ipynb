{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building Process\n",
    "\n",
    "### 1. Train-Test-Split\n",
    "- Random Split (Representativeness)\n",
    "\n",
    "#### Training Set: \n",
    "1. **Exploratory Analysis** (Descriptive, Visualization)\n",
    "  - Understand your data, and get an idea of what to do with your data\n",
    "\n",
    "\n",
    "2. **Pre-processing** (fit/transform)\n",
    "  - Take the result of Exploratory Analysis and take action (e.g. scaling, standardize, 1-hot-encoding, etc)\n",
    "\n",
    "\n",
    "3. **Modeling** (fit/predict/evaluate)\n",
    "  - Modeling techniques (Linear Rrgression, Logsitic Regression, Decision Trees, Neural Network)\n",
    "  - Hyperparameter tuning\n",
    "\n",
    "\n",
    "4. **K-Fold Cross-Validation**\n",
    "\n",
    "---\n",
    "\n",
    "***Avoid Data Leakage*** - no back-and-forth between training and testing set\n",
    "\n",
    "---\n",
    "\n",
    "#### Testing Set:\n",
    "1. **Pre-processing** (transform)\n",
    "  - Replicate the exact same process and transform your testing set\n",
    "\n",
    "\n",
    "2. **Test Model** (predict/evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit/Transform vs Transform\n",
    "- train = [0, 50, 100, 150, 200]\n",
    "\n",
    "min-max scale:\n",
    "- (e - min)/(max - min)\n",
    "- min = 200\n",
    "- max = 0\n",
    "- return : mmstrain = [0, 0.25, 0.5, 0.75, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [0, 50, 100, 150, 200]\n",
    "\n",
    "from sklearn.preprocess import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# everything in SKLearn is class, needs to be instantiated\n",
    "\n",
    "# under pre-processing (fit/transform)\n",
    "mms.fit(train) # -> model fitting : extract parameter | min = 0, max = 200\n",
    "mms.transform(train) # -> based on the fitted parameter, tranform your data\n",
    "# train -> [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "# \n",
    "test = [0, 50, 100, 150, 2000]\n",
    "# min = 0, max = 2000 \n",
    "mms.transform(test) # -> [0, 0.25, 0.5, 0.75, 10] --> which does not align with the idea of MinMaxScaler\n",
    "# .predict(test) -> ok\n",
    "# .transform(test) -> ok\n",
    "# .fit(test) -> NOT OKAY!!\n",
    "\n",
    "# But it is ok for 3 reason (outlier)\n",
    "# # 1. Random Split - representative enough\n",
    "# # 2. Not overfitting to our data - robust model can take care of it\n",
    "# # 3. Expected - Analysts' jobs to maintain and re-train our model (perhaps something has fundamentally shiftted) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciKitLearn\n",
    "\n",
    "- Built on SciPy, NumPy, Matplotlib\n",
    "- Commercially Useable -> BSD License\n",
    "\n",
    "Resources:\n",
    "- https://scikit-learn.org/stable/index.html\n",
    "- https://scikit-learn.org/stable/user_guide.html\n",
    "- https://scikit-learn.org/stable/modules/classes.html\n",
    "\n",
    "## Datasets\n",
    "\n",
    "- [Toy Datasets](https://scikit-learn.org/stable/datasets/index.html#toy-datasets)\n",
    "- [Real World Datasets](https://scikit-learn.org/stable/datasets/index.html#real-world-datasets)\n",
    "- [Generated Datasets](https://scikit-learn.org/stable/datasets/index.html#generated-datasets)\n",
    "\n",
    "### Load Toy Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load iris data\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X, y) # array, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris() # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.DESCR) # information about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = dataset.data,\n",
    "                 columns = dataset.feature_names)\n",
    "df['label'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `load_wine` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "dataset = load_wine()\n",
    "df = pd.DataFrame(data = dataset.data,\n",
    "                 columns = dataset.feature_names)\n",
    "df['label'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `load_breast_cancer` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "df = pd.DataFrame(data = dataset.data,\n",
    "                 columns = dataset.feature_names)\n",
    "df['label'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `load_diabetes` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "dataset = load_diabetes()\n",
    "df = pd.DataFrame(data = dataset.data,\n",
    "                 columns = dataset.feature_names)\n",
    "df['label'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `load_boston` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()\n",
    "df = pd.DataFrame(data = dataset.data,\n",
    "                 columns = dataset.feature_names)\n",
    "df['label'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated dataset\n",
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# load\n",
    "X, y = make_regression(n_samples = 300,\n",
    "                       n_features = 1,\n",
    "                       noise = 5)\n",
    "# plot\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# load\n",
    "X, y = make_regression(n_samples = 300,\n",
    "                       n_features = 1,\n",
    "                       noise = 5)\n",
    "\n",
    "# polynomial \n",
    "y = y*y\n",
    "\n",
    "# plot\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate isotropic Gaussian blobs for clustering\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# load\n",
    "X, y = make_blobs(n_samples=100,\n",
    "                 n_features=2,\n",
    "                 centers=3,\n",
    "                 cluster_std=1,\n",
    "                 random_state=3)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], marker='o', c=y, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_circle\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# load\n",
    "X, y = make_circles(noise=0.2,\n",
    "                    factor=0.5,\n",
    "                   random_state=1)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], marker='o', c=y, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_moon\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# load\n",
    "X, y = make_moons(n_samples= 100, noise=0.1)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], marker='o', c=y, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_classification\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# load\n",
    "X, y = make_classification(n_samples= 1000,\n",
    "                           n_features = 2,\n",
    "                           n_redundant = 0,\n",
    "                           n_informative = 1, # -> 1\n",
    "                           n_clusters_per_class = 1,\n",
    "                           n_classes = 2)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], marker='o', c=y, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_classification\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# load\n",
    "X, y = make_classification(n_samples= 1000,\n",
    "                           n_features = 2,\n",
    "                           n_redundant = 0,\n",
    "                           n_informative = 2,\n",
    "                           n_clusters_per_class = 2, # ->2\n",
    "                           n_classes = 2)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], marker='o', c=y, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_classification\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# load\n",
    "X, y = make_classification(n_samples= 1000,\n",
    "                           n_features = 2,\n",
    "                           n_redundant = 0,\n",
    "                           n_informative = 2,\n",
    "                           n_clusters_per_class = 1, # ->2\n",
    "                           n_classes = 3)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], marker='o', c=y, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate isotropic Gaussian and label samples by quantile\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# load\n",
    "X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X1[:,0], X1[:,1], marker='o', c=Y1, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "- Documentation:\n",
    "  - https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "\n",
    "- Encoding Categorical Variables:\n",
    "  - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "\n",
    "- Transforming Prediction Targets:\n",
    "  - https://scikit-learn.org/stable/modules/preprocessing_targets.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\n",
    "\n",
    "\n",
    "- Standardization, Scaling, Normalization:\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "\n",
    "- Discretization:\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html\n",
    "\n",
    "\n",
    "- Missing Value Imputation\n",
    "  - https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "\n",
    "- Polynomial Features\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "\n",
    "- Custom Transformers\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html\n",
    "\n",
    "\n",
    "## Process(`OrdinalEncoder`)\n",
    "- fit and transform training data\n",
    "- transform testing data\n",
    "\n",
    "### Fit and Transforming The Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "train = pd.DataFrame(\n",
    "    data=[\n",
    "    [1, 'Aaron', 'Third', 22, 7.25, 'no'], \n",
    "    [2, 'Beth', 'First', 38, 71.28, 'yes'], \n",
    "    [3, 'Cathy', 'Second', 26, 7.92, 'yes'], \n",
    "    [4, 'Dave', 'First', 60, 71.28, 'yes'], \n",
    "    [5, 'Erin', 'Second', 70, 71.92, 'no']], \n",
    "    columns=['Id', 'Name', 'Pclass', 'Age', 'Fare', 'Survived'])\n",
    "\n",
    "# feature\n",
    "train_X = train.drop('Survived', axis=1)\n",
    "\n",
    "# target\n",
    "train_Y = train['Survived']\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical variable - ordinal encoding\n",
    "# (not one-hot-encoding)\n",
    "\n",
    "# import\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the preprocess model\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "# fit model to training data w/ preprocessing model\n",
    "# feed training data\n",
    "oe.fit(train_X[['Pclass']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training data and \n",
    "oe.transform(train_X[['Pclass']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into a new column\n",
    "train_X['Pclass_transformed'] = oe.transform(train_X[['Pclass']])\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original data\n",
    "train_X.drop('Pclass', axis=1, inplace=True)\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Test Data\n",
    "\n",
    "Now, that we have a model that transform original data to new type of data.\n",
    "We can use `transform` on our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(\n",
    "    data=[\n",
    "    [6, 'Fiona', 'Second', 2, 50.25, 'yes'], \n",
    "    [7, 'Gina', 'Third', 25, 7.28, 'no'], \n",
    "    [8, 'Heather', 'First', 30, 71.92, 'no'], \n",
    "    [9, 'Ingrid', 'First', 54, 71.28, 'yes'], \n",
    "    [10, 'John', 'Third', 66, 7.92, 'yes']], \n",
    "    columns=['Id', 'Name', 'Pclass', 'Age', 'Fare', 'Survived'])\n",
    "\n",
    "test_X = test.drop('Survived', axis=1)\n",
    "\n",
    "test_Y = test['Survived']\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['Plcass_transformed'] = oe.transform(test_X[['Pclass']])\n",
    "\n",
    "test_X.drop('Pclass', axis=1, inplace=True)\n",
    "\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also combine fit and transform into one step!\n",
    "\n",
    "# sample data\n",
    "train = pd.DataFrame(\n",
    "    data=[\n",
    "    [1, 'Aaron', 'Third', 22, 7.25, 'no'], \n",
    "    [2, 'Beth', 'First', 38, 71.28, 'yes'], \n",
    "    [3, 'Cathy', 'Second', 26, 7.92, 'yes'], \n",
    "    [4, 'Dave', 'First', 60, 71.28, 'yes'], \n",
    "    [5, 'Erin', 'Second', 70, 71.92, 'no']], \n",
    "    columns=['Id', 'Name', 'Pclass', 'Age', 'Fare', 'Survived'])\n",
    "\n",
    "# feature\n",
    "train_X = train.drop('Survived', axis=1)\n",
    "\n",
    "# target\n",
    "train_Y = train['Survived']\n",
    "\n",
    "\n",
    "# import\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# instantiate the preprocess model\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "# fit + transform\n",
    "train_X['Pclass_transformed'] = oe.fit_transform(train_X[['Pclass']])\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have fit and transformed the training data, let's transform our testing data\n",
    "test = pd.DataFrame(\n",
    "    data=[\n",
    "    [6, 'Fiona', 'Second', 2, 50.25, 'yes'], \n",
    "    [7, 'Gina', 'Third', 25, 7.28, 'no'], \n",
    "    [8, 'Heather', 'First', 30, 71.92, 'no'], \n",
    "    [9, 'Ingrid', 'First', 54, 71.28, 'yes'], \n",
    "    [10, 'John', 'Third', 66, 7.92, 'yes']], \n",
    "    columns=['Id', 'Name', 'Pclass', 'Age', 'Fare', 'Survived'])\n",
    "\n",
    "test_X = test.drop('Survived', axis=1)\n",
    "\n",
    "test_Y = test['Survived']\n",
    "\n",
    "\n",
    "test_X['Pclass_transformed'] = oe.transform(test_X[['Pclass']])\n",
    "\n",
    "test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories='auto',\n",
    "                   drop=None, # None: dont drop any column e.g. typically drop 1\n",
    "                   handle_unknown='error', # category not exist in training data but in testing data, # typically ignore\n",
    "                   sparse=False, # True by default. used with pyspark\n",
    "                   dtype=int) # default float\n",
    "\n",
    "\n",
    "ohe.fit(df[['Pclass']])\n",
    "\n",
    "ohe.transform(df[['Pclass']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names()\n",
    "# could used to plug into original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could of course fit and transform in one step\n",
    "dfcat =  pd.DataFrame(ohe.fit_transform(df[['Pclass']]),\n",
    "                     columns = ohe.get_feature_names())\n",
    "dfcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcat = pd.concat([df, dfcat], axis=1)\n",
    "dfcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the first encoded column\n",
    "\n",
    "ohe = OneHotEncoder(categories='auto',\n",
    "                   drop='first', \n",
    "                   handle_unknown='error',\n",
    "                   sparse=False,\n",
    "                   dtype=int)\n",
    "\n",
    "dfcat =  pd.DataFrame(ohe.fit_transform(df[['Pclass']]),\n",
    "                     columns = ohe.get_feature_names())\n",
    "\n",
    "dfcat = pd.concat([df, dfcat], axis=1)\n",
    "\n",
    "dfcat = dfcat.drop('Pclass', axis=1)\n",
    "\n",
    "dfcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_var = ['Pclass','Sex','Embarked']\n",
    "\n",
    "ohe = OneHotEncoder(categories='auto',\n",
    "                   drop='first',\n",
    "                   handle_unknown='error',\n",
    "                   sparse=False,\n",
    "                   dtype=int)\n",
    "\n",
    "df = df.fillna('X')\n",
    "\n",
    "dfcat =  pd.DataFrame(ohe.fit_transform(df[categorical_var]),\n",
    "                     columns = ohe.get_feature_names())\n",
    "\n",
    "dfcat = pd.concat([df, dfcat], axis=1)\n",
    "\n",
    "dfcat = dfcat.drop(categorical_var, axis=1)\n",
    "\n",
    "dfcat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing, Scaling, Normalizing\n",
    "\n",
    "### Standardization - Mean Removal and Variance Scaling\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "  - `xi_scale = (xi - xmean)/xsd`\n",
    "  - resulting distribution has mean 0 and sd 1.\n",
    "  - sensitive to outliers, and cannot guarantee balanced scales in the presence of outliers.\n",
    "  - also, the outliers themselves are still present in the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_vars = ['Age', 'Fare']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler(with_mean=True,\n",
    "                   with_std=True)\n",
    "\n",
    "dfnumss = pd.DataFrame(ss.fit_transform(df[numeric_vars]),\n",
    "                      columns = ['ss_'+x for x in numeric_vars])\n",
    "\n",
    "dfnumss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, dfnumss], axis=1).drop(numeric_vars, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ss_Age'].mean(), df['ss_Age'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ss_Fare'].mean(), df['ss_Fare'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scale - scaling each feature to a given range (by default [0,1])\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "  - `xi_scale = (xi - min(x))/(max(x) - min(x))`\n",
    "  - by default, resulting distribution is in [0, 1] range.\n",
    "  - **sensitive to outliers**, and cannot guarantee balanced scales in the presence of outliers.\n",
    "  - also, the outliers themselves are still present in the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "numeric_vars = ['Age', 'Fare']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "dfnummms = pd.DataFrame(mms.fit_transform(df[numeric_vars]),\n",
    "                      columns = ['mms_'+x for x in numeric_vars])\n",
    "\n",
    "df = pd.concat([df, dfnummms], axis=1).drop(numeric_vars, axis=1)\n",
    "\n",
    "dfnummms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mms_Age'].min() , df['mms_Age'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mms_Fare'].min() , df['mms_Fare'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the result of transforming the testing set is not going to be exact\n",
    "# the whole point is to see how robust and general the model is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAbsScaler\n",
    "  - MaxAbsScaler scales each feature by its maximum absolute value.\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\n",
    "  - `xi_scale = (xi)/(max(abs(x)))`\n",
    "  - resulting distribution is in [-1, 1] range.\n",
    "  - **sensitive to outliers**, and cannot guarantee balanced scales in the presence of outliers.\n",
    "  - also, the outliers themselves are still present in the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "numeric_vars = ['Age', 'Fare']\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "mas = MaxAbsScaler()\n",
    "\n",
    "dfnummas = pd.DataFrame(mas.fit_transform(df[numeric_vars]),\n",
    "                      columns = ['mas_'+x for x in numeric_vars])\n",
    "\n",
    "dfnummas = pd.concat([dfnummas, df], axis=1).drop(numeric_vars, axis=1)\n",
    "\n",
    "dfnummas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\n",
    "  - removes the median and scales the data according to the inter-quartile range (defaults to IQR (or Q3-Q1))\n",
    "  - `xi_scale = (xi - Q2(x))/(Q3(x) - Q1(x))` where Q1, Q2, and Q3 are 25th, 50th and 75th quantiles\n",
    "  - robust to outliers, but the outliers themselves are still present in the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "numeric_vars = ['Age', 'Fare']\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler()\n",
    "\n",
    "dfnumrs = pd.DataFrame(rs.fit_transform(df[numeric_vars]),\n",
    "                      columns = ['rs_'+x for x in numeric_vars])\n",
    "\n",
    "dfnumrs = pd.concat([dfnumrs, df], axis=1).drop(numeric_vars, axis=1)\n",
    "\n",
    "dfnumrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Transformer\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer\n",
    "  - makes data more **Gaussian-like**.\n",
    "  - finds the optimal scaling factor to stabilize variance and mimimize skewness through maximum likelihood estimation. \n",
    "  - by default, PowerTransformer also applies zero-mean, unit variance normalization to the transformed output. \n",
    "  - supports the Box-Cox transform (can only be applied to strictly positive data) and the **Yeo-Johnson** transform (if there are negative values in data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "numeric_vars = ['Age', 'Fare']\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "pt = PowerTransformer()\n",
    "dfnumpt = pd.DataFrame(pt.fit_transform(df[numeric_vars]), columns=['pt_'+x for x in numericvars])\n",
    "dfnumpt = pd.concat([df, dfnumpt], axis=1).drop(numericvars, axis=1)\n",
    "dfnumpt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (Sample Vector)\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "- Normalization is the process of scaling **individual *samples***( **not features** - i.e., operation is along **rows**!) to have unit norm. \n",
    "- This process can be useful if you plan to use a quadratic form such as the dot-product \n",
    "- or any other kernel to quantify the similarity of any pair of samples.\n",
    "  - l1: sum of abs values is 1\n",
    "  - **l2: sum of square of values is 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris for all neumeric data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data = iris.data, columns = iris.feature_names)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm = Normalizer(norm='l2')\n",
    "\n",
    "dfnorm = pd.DataFrame(norm.fit_transform(df), columns=['norm_'+x for x in df.columns])\n",
    "\n",
    "dfnorm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dfnorm.apply(lambda x: x**2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCRETIZATION (or quantization or binning)\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html\n",
    "\n",
    "\n",
    "### KBinsDiscretizer: bin continuous data into intervals\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# instantiate\n",
    "kbd = KBinsDiscretizer(n_bins=5,            # bins size\n",
    "                       encode='ordinal',    # default : onehot\n",
    "                       strategy='quantile') # read documentation for encode and strategy\n",
    "\n",
    "# fit, transform\n",
    "dfkbd = pd.DataFrame(kbd.fit_transform(df), columns=['kbd_'+x for x in df.columns])\n",
    "\n",
    "dfkbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the bin boundaries \n",
    "kbd.bin_edges_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizer: \n",
    "  - binarize data (set feature values to 0 or 1) according to a threshold\n",
    "  - Binarizer is similar to the KBinsDiscretizer when k = 2, and when the bin edge is at the value threshold.\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "bnr = Binarizer(threshold=4.9)\n",
    "dfbnr = pd.DataFrame(bnr.fit_transform(df[['sepal length (cm)']]), columns=['bnr_sepal length'])\n",
    "dfbnr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Imputation\n",
    "\n",
    "- Missing Value Imputation\n",
    "  - https://scikit-learn.org/stable/modules/impute.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html\n",
    "\n",
    "\n",
    "### Univariate Feature Imputation\n",
    "  - https://scikit-learn.org/stable/modules/impute.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# impute Age missing value with mean\n",
    "impage = SimpleImputer(missing_values=np.NaN,\n",
    "                       strategy='mean',\n",
    "                       fill_value=None)\n",
    "\n",
    "df_impage = pd.DataFrame(data=impage.fit_transform(df[['Age']]),\n",
    "                         columns=['imp_Age'])\n",
    "df_impage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute Age missing value with mean\n",
    "impcabin = SimpleImputer(missing_values=np.NaN,\n",
    "                       strategy='constant',\n",
    "                       fill_value='MISSING')\n",
    "\n",
    "df_impcabin = pd.DataFrame(data=impcabin.fit_transform(df[['Cabin']]),\n",
    "                         columns=['imp_Cabin'])\n",
    "df_impcabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfimp = pd.concat([df, df_impage,df_impcabin], axis=1)\n",
    "dfimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "dfimp[(df['Age'].isna()) | (df['Cabin'].isna())].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfimp = dfimp.drop(['Age', 'Cabin'],axis=1)\n",
    "dfimp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate feature imputation\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n",
    "\n",
    "\n",
    "#### NOTE!!! \n",
    "- This estimator is still **experimental for now**: the predictions and the API might change without any deprecation cycle. So to use it, **you need to explicitly** `import enable_iterative_imputer`.\n",
    "- Each feature with missing values is modeled as a function of other features, and that estimate is then used for imputation. \n",
    "- It achieves this an iterated round-robin fashion: \n",
    "- At each step, a feature column is designated as output y, and the other feature columns are treated as inputs X. \n",
    "- A regressor is fit on (X, y) for known y. Then, the regressor is used to **predict the missing values** of y. \n",
    "- This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. \n",
    "- The results of the final imputation round are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "numericvars = ['SibSp', 'Parch', 'Age']\n",
    "\n",
    "imp = IterativeImputer(estimator = None, # which means using the default estimator : 'BayesianRidge()'\n",
    "                      max_iter=10,\n",
    "                      random_state=0)\n",
    "\n",
    "dfimp = pd.DataFrame(data = imp.fit_transform(df[numericvars]),\n",
    "                     columns = ['imp_'+x for x in numericvars])\n",
    "\n",
    "dfimp = pd.concat([df,dfimp], axis=1)\n",
    "\n",
    "dfimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# use RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "numericvars = ['SibSp', 'Parch', 'Age']\n",
    "\n",
    "imp = IterativeImputer(estimator = RandomForestRegressor(),\n",
    "                      max_iter=10,\n",
    "                      random_state=0)\n",
    "\n",
    "dfimp = pd.DataFrame(data = imp.fit_transform(df[numericvars]),\n",
    "                     columns = ['imp_'+x for x in numericvars])\n",
    "\n",
    "dfimp = pd.concat([df,dfimp], axis=1)\n",
    "\n",
    "dfimp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Indicator\n",
    "\n",
    "- When using imputation, preserving the information about which values had been missing can be informative. \n",
    "- Can use MissingIndicator to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "lst = [[1, 2, 20], [3, 6, 60], [4, 8, 80], [np.nan, 3, 30], [np.nan, np.nan, 70]]\n",
    "dff = pd.DataFrame(lst)\n",
    "\n",
    "mi = MissingIndicator(missing_values=np.NaN)\n",
    "\n",
    "mi.fit_transform(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLYNOMIAL FEATURE GENERATION\n",
    "- Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree.\n",
    "- For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df = df[['sepal length (cm)','sepal width (cm)']]\n",
    "df.rename(columns={'sepal length (cm)':'a', 'sepal width (cm)':'b'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2,\n",
    "                         interaction_only=False, # If true, only interaction features are produced: features that are products\n",
    "                         include_bias=True) # intercept\n",
    "\n",
    "dfpoly = pd.DataFrame(data=poly.fit_transform(df),\n",
    "                     columns= ['bias', 'a', 'b', 'a^2', 'ab', 'b^2'])\n",
    "\n",
    "dfpoly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUSTOM TRANSFORMERS \n",
    "- A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. \n",
    "- This is useful for stateless transformations such as taking the log of frequencies, doing custom scaling, etc.\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "ft = FunctionTransformer(np.log, # function to run\n",
    "                        validate=True) # throw error when something weird happens\n",
    "\n",
    "dfft = pd.DataFrame(data= ft.fit_transform(df),\n",
    "                   columns= ['ft_'+ x for x in df.columns])\n",
    "\n",
    "dfft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMING PREDICTION TARGET\n",
    "- These are transformers that are not intended to be used on features, but only on supervised learning targets.\n",
    "- https://scikit-learn.org/stable/modules/preprocessing_targets.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv')\n",
    "df.columns = ['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)','iris species']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(frac=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iris species'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "- (similar to OrdinalEncoder for Categorical Features)\n",
    "- use LabelEncoder to Encode labels with value between 0 and n_classes-1\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dfle = pd.DataFrame(le.fit_transform(df['iris species']), columns=['iris species LE'])\n",
    "dfle = pd.concat([df, dfle],axis=1).drop('iris species',axis=1)\n",
    "dfle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Binarization \n",
    "- similar to **OneHotEncoder** for Categorical Features\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "dflb = pd.DataFrame(lb.fit_transform(df['iris species']),\n",
    "                    columns=[x+' LB' for x in lb.classes_]) # suffix with LB by the end of species names\n",
    "dflb = pd.concat([df, dflb], axis=1).drop('iris species', axis=1)\n",
    "dflb.sample(frac=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel Binarizer \n",
    "- converts lists of sets or tuples into multilabel format\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = [('sci-fi', 'thriller'), # first movie genre\n",
    "          ('comedy',)]  # second movie genre\n",
    "pd.DataFrame(mlb.fit_transform(labels), columns = mlb.classes_) # columns names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "The sklearn ML API is very consistent:\n",
    "\n",
    "0. read data\n",
    "1. explore data\n",
    "2. preprocess data\n",
    "3. setup data for consumption by ML model \n",
    "    4. (4) choose the model by importing the appropriate estiamtor class from sklearn [from sklearn import model]\n",
    "    5. (5) instantiate the model with desired parameter values [ml=model()]\n",
    "    6. (6) fit the model to the training data [ml.fit(Xtrain, ytrain)]\n",
    "    7. (7) apply the model to test data [ypred=ml.predict(Xtest) or ml.transform(Xtest)]\n",
    "8. evaluate model\n",
    "9. deploy/use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) read data\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(data= boston.data, columns= boston.feature_names)\n",
    "df['label'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) explore data\n",
    "# not demonstrating for this example\n",
    "\n",
    "# 2) preprocess data\n",
    "# not demonstrating for this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) set up for ML model\n",
    "X = df.drop('label',axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# instantiate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2, # 20% in testing set\n",
    "                                                    random_state=1) # replicate the result, almost will not be using it tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) choose the model by importing the appropriate estiamtor class from sklearn [from sklearn import model]\n",
    "# Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # note this is a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) instantiate the model with desired parameter values [ml=model()]\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) fit the model to the training data [ml.fit(Xtrain, ytrain)]\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) apply the model to test data [ypred=ml.predict(Xtest) or ml.transform(Xtest)]\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Model Evaluation Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) evaluate model\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(np.sqrt( metrics.mean_squared_error( y_test, y_pred ) )) # -> rmse\n",
    "print(metrics.r2_score( y_test, y_pred)) # rsquare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predicted and actual result\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.jointplot(y_pred, y_test)\n",
    "sns.jointplot(y_pred, (y_test-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) deploy/use model\n",
    "lr.predict([[0.03237, 0.0, 2.18, 0.0, 0.458, 6.998, 45.8, 6.0622, 3.0, 222.0, 18.7, 394.63, 2.94]])\n",
    "# sample from training set; expected label 33.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Linear Model\n",
    "\n",
    "### Ridge Regression\n",
    "- https://scikit-learn.org/stable/modules/linear_model.html\n",
    "- Least squares with L2 regularization:\n",
    "- minmize ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
    "- L2 norm here imposes a penalty on the size of the coefficients, the larger the value of alpha, \n",
    "- the greater the amount of shrinkage and thus the coefficients become more robust to collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rr = Ridge(alpha=1.0)\n",
    "rr.fit(X_train, y_train)\n",
    "y_pred = rr.predict(X_test)  \n",
    "\n",
    "from sklearn import metrics\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # rmse\n",
    "print(metrics.r2_score(y_test, y_pred)) # r-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "- https://scikit-learn.org/stable/modules/linear_model.html\n",
    "- Least squares with L1 regularization:\n",
    "- minimize (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
    "- L1 norm here causes the model to prefer solutions with fewer non-zero coefficients, \n",
    "- effectively reducing the number of features upon which the given solution is dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lr = Lasso(alpha=1.0)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)  \n",
    "\n",
    "from sklearn import metrics\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # rmse\n",
    "print(metrics.r2_score(y_test, y_pred)) # r-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression\n",
    "- https://scikit-learn.org/stable/modules/linear_model.html\n",
    "- Least squares with L1 and L2 regularization:\n",
    "- minimize 1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet()\n",
    "en.fit(X_train, y_train)\n",
    "y_pred = en.predict(X_test)  \n",
    "\n",
    "from sklearn import metrics\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # rmse\n",
    "print(metrics.r2_score(y_test, y_pred)) # r-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples = 300, n_features=1, noise=5)\n",
    "y = y*y\n",
    "\n",
    "# fit linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "ypred = lr.predict(X)\n",
    "\n",
    "#shold be bad since we're fitting on a linear regression\n",
    "\n",
    "\n",
    "# now, lets fit polynomial regression model - by using polynomial features with linear regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2) # n^2\n",
    "\n",
    "Xpoly = poly.fit_transform(X)\n",
    "\n",
    "pr = LinearRegression()\n",
    "\n",
    "pr.fit(Xpoly, y)\n",
    "\n",
    "yppred = pr.predict(Xpoly) # not sorted\n",
    "\n",
    "sortedX, sortedyppred = zip(*sorted(zip(X, yppred))) # first zip, then sort, then unzip\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "# original\n",
    "plt.scatter(X, y)\n",
    "# ypred (lr)\n",
    "plt.plot(X, ypred, 'r-')\n",
    "# sorted yppred\n",
    "plt.plot(sortedX, sortedyppred, 'g-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "\n",
    "### Logisitc Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Logistic Regression\n",
    "#\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#\n",
    " \n",
    "# 0) read data\n",
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['label'] = dataset.target\n",
    "df.head()\n",
    "\n",
    "# 1) explore data\n",
    "# not demonstrating for this example\n",
    "\n",
    "# 2) preprocess data\n",
    "# not demonstrating for this example\n",
    "\n",
    "# 3) setup data for ml model\n",
    "X = df.drop(['label'], axis=1)\n",
    "y = df['label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression                                                                   # 4) choose the model \n",
    "lr = LogisticRegression(C=1e5, solver='lbfgs', multi_class='auto', class_weight=None)                # 5) instantiate the model \n",
    "lr.fit(X_train, y_train)                                                                                                                         # 6) fit the model to train data\n",
    "y_pred = lr.predict(X_test)                                                                                                                # 7) apply model to test data \n",
    "\n",
    "# 8) evaluate model\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Neighbor Classification\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/neighbors.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# simple majority vote (weights='uniform') of 5 nearest neighbors (n_neighbors=5) based on euclidean distance (p=2, metric='minkowski')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,\n",
    "                           weights='uniform',\n",
    "                           p=2, metric='minkowski')\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, we can use the \"elbow method\" to pick an optimal k\n",
    "# check: gradient descent\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "errorlst = pd.DataFrame(data=None, columns=['k','error'])\n",
    "\n",
    "for k in range(1,5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    error = metrics.mean_absolute_error(y_test, y_pred) # record the error\n",
    "    errorlst = errorlst.append ({'k':k, 'error':error}, ignore_index=True) # record the k value and error value\n",
    "\n",
    "# plot the record\n",
    "plt.plot (errorlst['k'], errorlst['error'], 'o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classification\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/svm.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- https://scikit-learn.org/stable/modules/svm.html#svm-kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=1.0, # regularization parameter\n",
    "          kernel='linear',\n",
    "          class_weight=None)\n",
    "\n",
    "# svc = SVC(C=1.0, kernel='rbf', gamma=0.7)\n",
    "# svc = SVC(C=1.0, kernel='poly', degree=3)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nb = MultinomialNB(alpha=1.0,\n",
    "#                    fit_prior=True,\n",
    "#                    class_prior=None)\n",
    "\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# nb = BernoulliNB()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# 8) evaluate model\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classification\n",
    "\n",
    " - https://scikit-learn.org/stable/modules/tree.html\n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier            \n",
    "dt = DecisionTreeClassifier(max_depth=None,\n",
    "                            min_samples_split=2,\n",
    "                            min_samples_leaf=1,\n",
    "                            class_weight=None)                                           \n",
    "dt.fit(X_train, y_train)                                                       \n",
    "y_pred = dt.predict(X_test)                                             \n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods - Bagging, Boosting & Stacking\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "- The goal of ensemble methods is to combine the predictions of several base estimators \n",
    "   built with a given learning algorithm in order to improve generalizability / robustness over \n",
    "   that of a single estimator.\n",
    "\n",
    "- In Averaging Methods, the driving principle is to build several estimators independently \n",
    "  and then to average their predictions. On average, the combined estimator is usually better \n",
    "  than any of the single base estimator because its variance is reduced. <br>\n",
    "  Examples include: \n",
    "   - Bagging (Bootrstrap Aggregation) Methods \n",
    "   - Forests of Randomized Trees (Random Forest, and Extremely Randomized (Extra) Trees)\n",
    "\n",
    "- By contrast, in Boosting Methods, base estimators are built sequentially and one tries to \n",
    "  reduce the bias of the combined estimator. The motivation is to combine several weak \n",
    "  models to produce a powerful ensemble.  <br>\n",
    "  Examples include: \n",
    "   - AdaBoost (Adaptive Boosting)\n",
    "   - Gradient Tree Boosting\n",
    "\n",
    "- Bagging methods work best with strong and complex models (e.g., fully developed decision \n",
    "  trees), in contrast with Boosting methods which usually work best with weak models \n",
    "  (e.g., shallow decision trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=1000,\n",
    "                 n_features=10,\n",
    "                 centers=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging ensemble of KNeighborsClassifier base estimators, \n",
    "# each built on random subsets of 50% of the samples drawn with replacement,\n",
    "# and 50% of the features drawn without replacement\n",
    "\n",
    "bc = BaggingClassifier(base_estimator = KNeighborsClassifier(),\n",
    "                      max_samples = 0.5, bootstrap=True,\n",
    "                      max_features = 0.5, bootstrap_features=False)\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "print(bc.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Forests of Randomized Trees\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- The sklearn.ensemble module includes two averaging algorithms based on randomized \n",
    "  decision trees: the RandomForest algorithm and the Extra-Trees method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10,\n",
    "                            max_features='auto',\n",
    "                            bootstrap=True,\n",
    "                            max_depth=None,\n",
    "                            min_samples_split=2,\n",
    "                            min_samples_leaf=1,\n",
    "                            class_weight=None)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(rfc.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Classifier\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n",
    "- Randomized Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, instantiate, train, test model\n",
    "from sklearn.ensemble import ExtraTreesClassifier  \n",
    "etc = ExtraTreesClassifier(n_estimators=10, \n",
    "                           max_features='auto', bootstrap=False,\n",
    "                           max_depth=None, min_samples_split=2, min_samples_leaf=1, class_weight=None)   \n",
    "\n",
    "etc.fit(X_train, y_train)\n",
    "y_pred = etc.predict(X_test)\n",
    "\n",
    "print (etc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost (Adaptive Boost)\n",
    " - Use abunch of weak estimators\n",
    " - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(n_estimators=100, learning_rate=0.01)\n",
    "\n",
    "abc.fit(X_train, y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "print (abc.score(X_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# svc = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier                                                                                                \n",
    "# abc = AdaBoostClassifier(base_estimator=svc,\n",
    "#                          n_estimators=100,\n",
    "#                          learning_rate=0.01,\n",
    "#                          algorithm='SAMME')\n",
    "\n",
    "# abc.fit(X_train, y_train)\n",
    "# y_pred = abc.predict(X_test)\n",
    "# print (abc.score(X_test, y_test)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tree Boosting\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01,\n",
    "                                                   max_depth=3, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "print (gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers \n",
    "- https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=10, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# import the classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# import the voting classifier\n",
    "from sklearn.ensemble import VotingClassifier   \n",
    "\n",
    "# instantiate\n",
    "dtc = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "svc = SVC(gamma='scale', kernel='rbf', probability=True)\n",
    "\n",
    "\n",
    "#eclf = VotingClassifier(estimators=[('dtc', dtc), ('knn', knn), ('svc', svc)], voting='hard')\n",
    "eclf = VotingClassifier(estimators=[('dtc', dtc), ('knn', knn), ('svc', svc)], voting='soft', weights=[2, 1, 2])\n",
    "\n",
    "eclf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = eclf.predict(X_test)\n",
    "\n",
    "print (eclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Methods\n",
    "\n",
    "### KMeans Clustering\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/clustering.html\n",
    "- https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=10000, n_features=2, centers=4, cluster_std=0.5 , random_state=0)\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = pd.DataFrame(data=None, columns=['K', 'Error'])\n",
    "\n",
    "for k in range(2,7):\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X, y)\n",
    "    error = km.inertia_\n",
    "    err = err.append({'K':k, 'Error':error, 'Silhouette':errorS}, ignore_index=True)\n",
    "\n",
    "plt.plot(err['K'], err['Error'], 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = pd.DataFrame(data=None, columns=['K', 'Silhouette'])\n",
    "\n",
    "for k in range(2,7):\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(X, y)\n",
    "    errorS = metrics.silhouette_score(X, km.labels_, metric='sqeuclidean')\n",
    "    err = err.append({'K':k, 'Silhouette':errorS}, ignore_index=True)\n",
    "\n",
    "plt.plot(err['K'], err['Silhouette'], 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, instantiate, train, test model\n",
    "from sklearn.cluster import KMeans                                         # 4) choose the model \n",
    "km = KMeans(n_clusters=4, random_state=0)                                  # 5) instantiate the model \n",
    "km.fit(X, y)                                                               # 6) fit the model to the training data\n",
    "\n",
    "error = km.inertia_                                                        # 8) evaluate the model\n",
    "#error = metrics.silhouette_score(X, km.labels_, metric='sqeuclidean') \n",
    "print (error)\n",
    "\n",
    "print (km.labels_)\n",
    "# center of each cluster\n",
    "print (km.cluster_centers_)\n",
    "\n",
    "# predict a new point will be in which cluster\n",
    "print (km.predict([[0,4]]))                                                # 9) deploy/use model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/decomposition.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- https://scikit-learn.org/stable/modules/unsupervised_reduction.html\n",
    "- PCA is sesative to relative scaling of the original data, so it's always a good idea to standardize before deploy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "dataset = load_breast_cancer()\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['label'] = dataset.target\n",
    "\n",
    "X = df.drop(['label'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing - standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler(with_mean=True, with_std=True)\n",
    "Xss = ss.fit_transform(X)\n",
    "\n",
    "# pca\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # 2 principle component\n",
    "Xpca = pca.fit_transform(Xss)\n",
    "Xpca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfXpca = pd.DataFrame(data=Xpca, columns=['PCA1', 'PCA2'])\n",
    "dfXpca['Cancer'] = y\n",
    "dfXpca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=dfXpca,\n",
    "          x='PCA1', y='PCA2',\n",
    "          hue='Cancer', fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 5\n",
    "\n",
    "### (I) Read data and pick relevant columns\n",
    "- read data\n",
    "- transform Cabin to Deck\n",
    "- only retain columns required for analysis\n",
    "\n",
    "#### Question 1\n",
    "- Read kaggle train.csv data into a dataframe called df; Then check the top few rows of the df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('train.csv')\n",
    "titanic.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "- Extract a new column called Deck from Cabin.\n",
    "- Hint: write a function called getDeck, and then use following:\n",
    "  - `df['Deck'] = df['Cabin'].apply(getDeck)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDeck(x):\n",
    "    if pd.notna(x):\n",
    "        return (x[0])\n",
    "    else:\n",
    "        return ('X')\n",
    "\n",
    "titanic['Deck'] = titanic['Cabin'].apply(getDeck)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (II) Split data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting data\n",
    "X = titanic.drop('Survived',axis=1)\n",
    "y = titanic['Survived']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "y_train = y_train.copy()\n",
    "y_test = y_test.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (III) Fit/Transform on Training Data\n",
    "\n",
    "- `'Age'`: impute missing values with median\n",
    "  - impute\n",
    "\n",
    "\n",
    "- `['Pclass', 'Sex', 'Deck']`: impute missing values with `'X'`\n",
    "  - impute\n",
    "\n",
    "\n",
    "- `['impPclass', 'impSex', 'impDeck']`: **One Hot Encoding**\n",
    "  - OneHotEncoding()\n",
    "\n",
    "\n",
    "- Only keep imputed numeric and ohe categorical features\n",
    "  - .drop\n",
    "\n",
    "\n",
    "- build Logistic Regression Model\n",
    "  - LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "#### Question 4\n",
    "- Set numeric_features = ['Age]\n",
    "- Use SimpleImputer to fill the missing values in numeric_features with the median values, and prefix the imputed columns with `imp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Age']\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "si_num = SimpleImputer(missing_values=np.NaN,\n",
    "                   strategy='median',\n",
    "                  fill_value=None)\n",
    "\n",
    "X_si_num = pd.DataFrame(si_num.fit_transform(X_train[numeric_features]),\n",
    "                        columns=['imp'+x for x in numeric_features],\n",
    "                        index=X_train.index)\n",
    "\n",
    "X_train = pd.concat([X_train, X_si_num],axis=1)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "- Set categorical_features = ['Pclass', 'Sex', 'Deck']\n",
    "- Use SimpleImputer to fill the missing values in categorical_features with the constant value X, and prefix the imputed columns with imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['Pclass', 'Sex', 'Deck']\n",
    "\n",
    "si_cat = SimpleImputer(missing_values=np.NaN,\n",
    "                   strategy='constant',\n",
    "                  fill_value='X')\n",
    "\n",
    "df_imp_cat = pd.DataFrame(data = si_cat.fit_transform(X_train[categorical_vars]),\n",
    "                         columns=['imp'+x for x in categorical_vars],\n",
    "                         index=X_train.index)\n",
    "\n",
    "X_train = pd.concat([X_train, df_imp_cat],axis=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "- Set imputed_categorical_features = [impPclass', impSex, impDeck']\n",
    "- Use OneHotEncoder to one-hot-encode the imputed categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_categorical_features = ['impPclass', 'impSex', 'impDeck']\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "OHE = OneHotEncoder(categories='auto',\n",
    "                   handle_unknown='ignore',\n",
    "                   sparse=False,\n",
    "                   dtype=int)\n",
    "\n",
    "df_cat =  pd.DataFrame(data = OHE.fit_transform(X_train[imputed_categorical_features]),\n",
    "                       columns = OHE.get_feature_names(),\n",
    "                       index= X_train.index)\n",
    "X_train = pd.concat([X_train, df_cat], axis=1)\n",
    "\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "- Only keep imputed numeric features, and ohe catergorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age',\n",
    "              'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', \n",
    "              'Deck', 'impPclass', 'impSex', 'impDeck',], axis=1,inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "Build Logistic Regression Model by fitting to the transformed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "logr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IV) Transform/Predict on Test Data\n",
    "- 'Age: impute missing values with median\n",
    "- ['Pclass', 'Sex', 'Deck]: impute missing values with X\n",
    "- [impPclass', impSex', 'impDeck']: OHE\n",
    "- Only keep imputed numeric and ohe categorical features\n",
    "- predict using Logistic Regression Model\n",
    "- evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric\n",
    "numeric_features = ['Age']\n",
    "\n",
    "X_si_num = pd.DataFrame(si_num.transform(X_test[numeric_features]),\n",
    "                        columns=['imp'+x for x in numeric_features],\n",
    "                        index=X_test.index)\n",
    "\n",
    "X_test = pd.concat([X_test, X_si_num],axis=1)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['Pclass', 'Sex', 'Deck']\n",
    "\n",
    "df_imp_cat = pd.DataFrame(data = si_cat.transform(X_test[categorical_vars]),\n",
    "                         columns=['imp'+x for x in categorical_vars],\n",
    "                         index=X_test.index)\n",
    "\n",
    "X_test = pd.concat([X_test, df_imp_cat],axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_categorical_features = ['impPclass', 'impSex', 'impDeck']\n",
    "\n",
    "df_cat =  pd.DataFrame(data = OHE.transform(X_test[imputed_categorical_features]),\n",
    "                       columns = OHE.get_feature_names(),\n",
    "                       index= X_test.index)\n",
    "\n",
    "X_test = pd.concat([X_test, df_cat], axis=1)\n",
    "\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.drop(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age',\n",
    "              'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', \n",
    "              'Deck', 'impPclass', 'impSex', 'impDeck',], axis=1,inplace=True)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this Exercise Set:\n",
    "\n",
    "- We had to keep track of the preprocessing/transformation steps with **\"fit_transform\"** on the training data (including remembering to drop the columns we didn't want to use anymore). and then repeating all of the same preprocessing/transformation steps with **\"transform\" on the test data**\n",
    "\n",
    "- We also had to keep track of building the model with **fit on the preprocessed/transformed *training data***. and then predicting with **predict on the preprocessed/transformed *test data*** As well see in the next exercise set, we can use ColumnTransformers and Pipelines to make life much-much easier for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5a\n",
    "\n",
    "## Pipelines with ColumnTransformers\n",
    "- https://scikit-learn.org/stable/modules/compose.html#pipeline\n",
    "- https://scikit-learn.org/stable/modules/compose.html#column-transformer\n",
    "\n",
    "\n",
    "## Pipeline\n",
    "Pipelines can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification.\n",
    "\n",
    "\n",
    "The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object.\n",
    "\n",
    "## ColumnTransformers\n",
    "\n",
    "The ColumnTransformer helps performing different transformations for different columns of the data, within a Pipeline that is safe from data leakage and that can be parametrized. ColumnTransformer works on arrays, sparse matrices, and pandas DataFrames.\n",
    "\n",
    "- Warning: The compose.ColumnTransformer class is experimental and the API is subject to change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "titanic = pd.read_csv('train.csv')\n",
    "\n",
    "# Get Useful Information while Handling the np.NaN \n",
    "def getDeck(x):\n",
    "    if pd.notna(x):\n",
    "        return (x[0])\n",
    "    else:\n",
    "        return (x)\n",
    "\n",
    "titanic['Deck'] = titanic['Cabin'].apply(getDeck)\n",
    "\n",
    "\n",
    "# spliting data into feature and target df\n",
    "X = titanic.drop('Survived',axis=1)\n",
    "y = titanic['Survived']\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "\n",
    "# create copy to deal with warning\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "y_train = y_train.copy()\n",
    "y_test = y_test.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up preprocessing pipeline for numeric data\n",
    "- impute missing values with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy - first pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_features = ['Age']\n",
    "\n",
    "# instantiate\n",
    "# steps : a list of TUPLE: [(,) , (,), ...]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values = np.nan, strategy='median'))]) # pipeline 1 - step 1\n",
    "\n",
    "# note: thing has happened right now, we are just setting the blue print what we want to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up preprocessing pipeline for categorical data\n",
    "- impute missing values with constant 'X'\n",
    "- one-hot-encode imputed categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex', 'Deck']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value='X')),          # pipeline 2 - step 1\n",
    "    ('ohe', OneHotEncoder(categories='auto',handle_unknown='ignore',sparse=False,dtype=int))])  # pipeline 2 - step 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up column transformer with preprocessing pipelines for numeric and categorical data\n",
    "- only keep imputed numeric and ohe catergorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental\n",
    "# copy\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# three item tuple ('name', #input, #pipeline)\n",
    "preprocessor = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                                ('num', numeric_transformer, numeric_features),\n",
    "                                ('cat', categorical_transformer, categorical_features)],\n",
    "                        remainder='drop') # only keep the preprocessed feature\n",
    "\n",
    "### remainder='passthrough'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the preprocessing->model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# nesting pipeline into yet another pipeline\n",
    "# pipeline run it sequentially\n",
    "# naming that taking the output as the input for next step\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('pp', preprocessor),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke clf\n",
    "- fit using the combined preprocessing and model pipeline on train data\n",
    "- this will automatically run **\"fit\" and \"transform\"** for all pre-processing steps, and \"fit\" for model step on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print()\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "print (metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5b\n",
    "\n",
    "## Cross-Validation\n",
    "- Note: Cross-Validation is typically done in conjunction with Grid-Search, as well see in (5d)\n",
    "- https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "A solution to this problem is a procedure called Cross-Validation. A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV.\n",
    "\n",
    "In the basic approach, called k-fold CV, the training set is split into k smaller sets. The following procedure is followed for each of the k folds:\n",
    "- A model is trained using k-1 of the folds as training data;\n",
    "- The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "\n",
    "\n",
    "## Hyper-Parameter Tuning\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikitlearn they are passed as arguments to the constructor of the estimator classes.\n",
    "\n",
    "\n",
    "When evaluating hyperparameters for estimators, there is a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can leak into the model and evaluation metrics no longer report on generalization performance.\n",
    "\n",
    "\n",
    "To solve this problem, yet another part of the dataset can be held out as a so-called validation set: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number\n",
    "of samples which can be used for learning the model, and the results can depend on a\n",
    "particular random choice for the pair of (train, validation) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up data and pipeline\n",
    "\n",
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "titanic = pd.read_csv('train.csv')\n",
    "\n",
    "# Get Useful Information while Handling the np.NaN \n",
    "def getDeck(x):\n",
    "    if pd.notna(x):\n",
    "        return (x[0])\n",
    "    else:\n",
    "        return (x)\n",
    "\n",
    "titanic['Deck'] = titanic['Cabin'].apply(getDeck)\n",
    "\n",
    "\n",
    "# spliting data into feature and target df\n",
    "X = titanic.drop('Survived',axis=1)\n",
    "y = titanic['Survived']\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "\n",
    "# create copy to deal with warning\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "y_train = y_train.copy()\n",
    "y_test = y_test.copy()\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_features = ['Age']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values = np.nan, strategy='median'))])\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex', 'Deck']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value='X')),\n",
    "    ('ohe', OneHotEncoder(categories='auto',handle_unknown='ignore',sparse=False,dtype=int))])\n",
    "\n",
    "# columns Transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                                ('num', numeric_transformer, numeric_features),\n",
    "                                ('cat', categorical_transformer, categorical_features)],\n",
    "                        remainder='drop')\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = Pipeline(steps=[\n",
    "    ('pp', preprocessor),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 5-fold cross-validation:\n",
    "- train, validate each time and get the mean scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77622378, 0.78321678, 0.81690141, 0.80985915, 0.82394366])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# cross_validate (pipeline, training_features, training_target, return_train_score)\n",
    "scores = cross_validate(clf,\n",
    "                        X_train, y_train,\n",
    "                        cv=5,\n",
    "                        return_train_score=False)\n",
    "\n",
    "# Return in Dictionary Structure\n",
    "scores['test_score'].mean()\n",
    "\n",
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5c\n",
    "\n",
    "## Grid-Search with Cross-Validation\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "It is possible and recommended to search the hyper-parameter space for the best cross validation score.\n",
    "\n",
    "\n",
    "Any parameter provided when constructing an estimator may be optimized in this manner. <br>\n",
    "A search consists of:\n",
    "- an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "- a parameter space;\n",
    "- a method for searching or sampling candidates;\n",
    "- a cross-validation scheme; and\n",
    "- a score function.\n",
    "\n",
    "\n",
    "GridSearchCV exhaustively considers all parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up data and pipeline\n",
    "\n",
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "titanic = pd.read_csv('train.csv')\n",
    "\n",
    "# Get Useful Information while Handling the np.NaN \n",
    "def getDeck(x):\n",
    "    if pd.notna(x):\n",
    "        return (x[0])\n",
    "    else:\n",
    "        return (x)\n",
    "\n",
    "titanic['Deck'] = titanic['Cabin'].apply(getDeck)\n",
    "\n",
    "\n",
    "# spliting data into feature and target df\n",
    "X = titanic.drop('Survived',axis=1)\n",
    "y = titanic['Survived']\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "\n",
    "# create copy to deal with warning\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "y_train = y_train.copy()\n",
    "y_test = y_test.copy()\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_features = ['Age']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values = np.nan, strategy='median'))])\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex', 'Deck']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value='X')),\n",
    "    ('ohe', OneHotEncoder(categories='auto',handle_unknown='ignore',sparse=False,dtype=int))])\n",
    "\n",
    "# columns Transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                                ('num', numeric_transformer, numeric_features),\n",
    "                                ('cat', categorical_transformer, categorical_features)],\n",
    "                        remainder='drop')\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = Pipeline(steps=[\n",
    "    ('pp', preprocessor),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))]) # lr will translate to lr__ for later use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = parameter grid (hyperparameter) --> in documentation (cmd + tab in, for example, LogisticRegression(**here**))\n",
    "\n",
    "# dictionary structure : \n",
    "## Key:\n",
    "##### lr__ (:logistic regression model we named earlier), the __ is a must\n",
    "##### can add as much hyperparameters as the model provide (in the following example)\n",
    "## Value: list of parameter that we want to use\n",
    "\n",
    "param_grid = {'lr__penalty': ['l1', 'l2']}\n",
    "\n",
    "gscv = GridSearchCV(estimator = clf,\n",
    "                    param_grid = param_grid,\n",
    "                    cv=5,\n",
    "                    return_train_score=False)\n",
    "\n",
    "# still a blue print for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('pp',\n",
      "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
      "                                   sparse_threshold=0.3,\n",
      "                                   transformer_weights=None,\n",
      "                                   transformers=[('num',\n",
      "                                                  Pipeline(memory=None,\n",
      "                                                           steps=[('si',\n",
      "                                                                   SimpleImputer(add_indicator=False,\n",
      "                                                                                 copy=True,\n",
      "                                                                                 fill_value=None,\n",
      "                                                                                 missing_values=nan,\n",
      "                                                                                 strategy='median',\n",
      "                                                                                 verbose=0))],\n",
      "                                                           verbose=False),\n",
      "                                                  ['Age']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(memory=None,\n",
      "                                                           steps=...\n",
      "                                                                                 handle_unknown='ignore',\n",
      "                                                                                 sparse=False))],\n",
      "                                                           verbose=False),\n",
      "                                                  ['Pclass', 'Sex', 'Deck'])],\n",
      "                                   verbose=False)),\n",
      "                ('lr',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "--------------------\n",
      "0.8020289569585344\n",
      "--------------------\n",
      "{'lr__penalty': 'l2'}\n",
      "--------------------\n",
      "{'mean_fit_time': array([0.01843162, 0.01651754]), 'std_fit_time': array([0.00119721, 0.001668  ]), 'mean_score_time': array([0.00903521, 0.0074707 ]), 'std_score_time': array([0.0013904 , 0.00086166]), 'param_lr__penalty': masked_array(data=['l1', 'l2'],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'lr__penalty': 'l1'}, {'lr__penalty': 'l2'}], 'split0_test_score': array([0.76923077, 0.77622378]), 'split1_test_score': array([0.78321678, 0.78321678]), 'split2_test_score': array([0.81690141, 0.81690141]), 'split3_test_score': array([0.79577465, 0.80985915]), 'split4_test_score': array([0.81690141, 0.82394366]), 'mean_test_score': array([0.796405  , 0.80202896]), 'std_test_score': array([0.01872416, 0.01888153]), 'rank_test_score': array([2, 1], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# Search for best params\n",
    "gscv.fit(X_train, y_train)\n",
    "print(gscv.best_estimator_)\n",
    "print('-'*50)\n",
    "print(gscv.best_score_)\n",
    "print('-'*50)\n",
    "print(gscv.best_params_)\n",
    "print('-'*50)\n",
    "print(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and Evaluate best_estimator_ on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7821229050279329\n",
      "--------------------------------------------------\n",
      "[[87 19]\n",
      " [20 53]]\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       106\n",
      "           1       0.74      0.73      0.73        73\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.77      0.77      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and Evaluate best_estimator_ on test data\n",
    "y_pred = gscv.best_estimator_.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print('-'*50)\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print('-'*50)\n",
    "print (metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to check if `strategy='median'` in the `numeric_transformer` pipeline is the best way: <br> \n",
    "`numeric_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values = np.nan, strategy='median'))])` <br>\n",
    "Say, we want to check both `'median'` and `'mean'`, How can we do it?\n",
    "\n",
    "\n",
    "#### 1) Remove the strategy = 'median'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up data and pipeline\n",
    "\n",
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# load data\n",
    "titanic = pd.read_csv('train.csv')\n",
    "\n",
    "# Get Useful Information while Handling the np.NaN \n",
    "def getDeck(x):\n",
    "    if pd.notna(x):\n",
    "        return (x[0])\n",
    "    else:\n",
    "        return (x)\n",
    "\n",
    "titanic['Deck'] = titanic['Cabin'].apply(getDeck)\n",
    "\n",
    "\n",
    "# spliting data into feature and target df\n",
    "X = titanic.drop('Survived',axis=1)\n",
    "y = titanic['Survived']\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)\n",
    "\n",
    "# create copy to deal with warning\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "y_train = y_train.copy()\n",
    "y_test = y_test.copy()\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "numeric_features = ['Age']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values = np.nan))]) ### <- HERE!!!\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex', 'Deck']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values=np.NaN, strategy='constant', fill_value='X')),\n",
    "    ('ohe', OneHotEncoder(categories='auto',handle_unknown='ignore',sparse=False,dtype=int))])\n",
    "\n",
    "# columns Transformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "                        transformers=[\n",
    "                                ('num', numeric_transformer, numeric_features),\n",
    "                                ('cat', categorical_transformer, categorical_features)],\n",
    "                        remainder='drop')\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = Pipeline(steps=[\n",
    "    ('pp', preprocessor),\n",
    "    ('lr', LogisticRegression(solver='liblinear'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'lr__penalty': ['l1', 'l2'],\n",
    "              'pp__num__si__strategy':['median','mean']} ### add it into the param_grid \n",
    "# following the naming convention (structure(level) of the pipelines we set up)\n",
    "# under 'pp' (preprocessing)\n",
    "#       >> 'num' (numeric_transformer)\n",
    "#           >> 'si' (simple imputer)\n",
    "#              >> 'strategy' (hyperparameter)\n",
    "\n",
    "gscv = GridSearchCV(clf,\n",
    "                    param_grid,\n",
    "                    cv=5,\n",
    "                    return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('pp',\n",
      "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
      "                                   sparse_threshold=0.3,\n",
      "                                   transformer_weights=None,\n",
      "                                   transformers=[('num',\n",
      "                                                  Pipeline(memory=None,\n",
      "                                                           steps=[('si',\n",
      "                                                                   SimpleImputer(add_indicator=False,\n",
      "                                                                                 copy=True,\n",
      "                                                                                 fill_value=None,\n",
      "                                                                                 missing_values=nan,\n",
      "                                                                                 strategy='median',\n",
      "                                                                                 verbose=0))],\n",
      "                                                           verbose=False),\n",
      "                                                  ['Age']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(memory=None,\n",
      "                                                           steps=...\n",
      "                                                                                 handle_unknown='ignore',\n",
      "                                                                                 sparse=False))],\n",
      "                                                           verbose=False),\n",
      "                                                  ['Pclass', 'Sex', 'Deck'])],\n",
      "                                   verbose=False)),\n",
      "                ('lr',\n",
      "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=100,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "--------------------------------------------------\n",
      "0.8020289569585344\n",
      "--------------------------------------------------\n",
      "{'lr__penalty': 'l2', 'pp__num__si__strategy': 'median'}\n",
      "--------------------------------------------------\n",
      "{'mean_fit_time': array([0.02090721, 0.01836991, 0.01678143, 0.01652932]), 'std_fit_time': array([0.00566371, 0.00234532, 0.00136411, 0.0015691 ]), 'mean_score_time': array([0.00783124, 0.00802631, 0.0080832 , 0.00702095]), 'std_score_time': array([0.00074691, 0.00061958, 0.00101199, 0.00132715]), 'param_lr__penalty': masked_array(data=['l1', 'l1', 'l2', 'l2'],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_pp__num__si__strategy': masked_array(data=['median', 'mean', 'median', 'mean'],\n",
      "             mask=[False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'lr__penalty': 'l1', 'pp__num__si__strategy': 'median'}, {'lr__penalty': 'l1', 'pp__num__si__strategy': 'mean'}, {'lr__penalty': 'l2', 'pp__num__si__strategy': 'median'}, {'lr__penalty': 'l2', 'pp__num__si__strategy': 'mean'}], 'split0_test_score': array([0.76923077, 0.76923077, 0.77622378, 0.77622378]), 'split1_test_score': array([0.78321678, 0.78321678, 0.78321678, 0.78321678]), 'split2_test_score': array([0.81690141, 0.81690141, 0.81690141, 0.80985915]), 'split3_test_score': array([0.79577465, 0.79577465, 0.80985915, 0.78873239]), 'split4_test_score': array([0.81690141, 0.81690141, 0.82394366, 0.82394366]), 'mean_test_score': array([0.796405  , 0.796405  , 0.80202896, 0.79639515]), 'std_test_score': array([0.01872416, 0.01872416, 0.01888153, 0.01777342]), 'rank_test_score': array([2, 2, 1, 4], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# Search for best params\n",
    "gscv.fit(X_train, y_train)\n",
    "print(gscv.best_estimator_)\n",
    "print('-'*50)\n",
    "print(gscv.best_score_)\n",
    "print('-'*50)\n",
    "print(gscv.best_params_)\n",
    "print('-'*50)\n",
    "print(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7821229050279329\n",
      "--------------------------------------------------\n",
      "[[87 19]\n",
      " [20 53]]\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       106\n",
      "           1       0.74      0.73      0.73        73\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.77      0.77      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and Evaluate best_estimator_ on test data\n",
    "y_pred = gscv.best_estimator_.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(y_test, y_pred))\n",
    "print('-'*50)\n",
    "print (metrics.confusion_matrix(y_test, y_pred))\n",
    "print('-'*50)\n",
    "print (metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
